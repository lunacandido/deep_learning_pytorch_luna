{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRvkvmryUsW7GDDxeqslR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunacandido/deep_learning_pytorch_luna/blob/main/Dataloader_trainingflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregamento de dados e Fluxo de treinamento"
      ],
      "metadata": {
        "id": "yN-J-omdv_2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O treinamento é um processo iterativo que segue o seguinte fluxo:\n",
        "\n",
        "Operar a entrada na rede, calcular função de perda, calcular o gradiente e atualizar os pesos,\n",
        "ou seja, consiste em passo de otimização.\n",
        "\n",
        "Batch é a quantidade de amostras vistas numa iteração. Já a época é quando todas as amostras do conjuntos de treino foram vistas pelo modelo, ou seja, iterou o suficiente pra ver todas as amostras de treino. E a cada nova época se refina o modelo.\n",
        "\n",
        "Obs: o gráfico de convergencia é definido com base na época."
      ],
      "metadata": {
        "id": "W8odKDBIxof6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn , optim\n",
        "## trabalhando com datasets de imagem\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms \n",
        "## importando matplot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "args = {\n",
        "    'batch_size': 20,\n",
        "    'num_workers' : 4,\n",
        "    'lr': 1e-4,\n",
        "    'weight_decay':5e-4,\n",
        "    'num_epochs': 2\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  args['device'] = torch.device('cuda')\n",
        "else:\n",
        "  args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])\n",
        "\n",
        "## importando o datasets de interesse\n",
        "treino_set = datasets.MNIST('./', \n",
        "                      train=True, \n",
        "                      transform=transforms.ToTensor(),\n",
        "                      download=True)\n",
        "teste_set = datasets.MNIST('./',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor(),\n",
        "                           download=False)\n",
        "\n",
        "print('Amostras de treino: ' + str(len(treino_set)) + '\\nAmostras de Teste:' + str(len(teste_set)))\n",
        "print(type(treino_set)) ## classe do datasets\n",
        "print(type(treino_set[0])) ## o item de qualquer dataset será uma tupla - (dado, rótulo)\n",
        "print(treino_set[0]) ## imprimindo a tupla para visualização\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "  dado, rotulo = treino_set[i]\n",
        "\n",
        "  plt.figure()\n",
        "  plt.imshow(dado[0])\n",
        "  plt.title('Rotulo: '+ str(rotulo))\n",
        "\n",
        "# transforms.RandomCrop(n) = realiza 'n' recortes na mesma imagem\n",
        "# basta trocar transforms.ToTensor() por transforms.RandomCrop(n)\n",
        "\n"
      ],
      "metadata": {
        "id": "oaYXLEFivzgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader\n",
        "\n",
        "- Separa os dados em batches\n",
        "- Embaralha os dados\n",
        "- Carrega batches em paralelo utilizando threads"
      ],
      "metadata": {
        "id": "_E2GwCwP5K5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importar o pacote do data loader\n",
        "from torch.utils.data import DataLoader\n",
        "## criando os batchs\n",
        "treino_loader = DataLoader(treino_set, \n",
        "                          batch_size=args['batch_size'], \n",
        "                          shuffle=True, # para embaralhar\n",
        "                          num_workers=args['num_workers'])\n",
        "\n",
        "teste_loader = DataLoader(teste_set, \n",
        "                          batch_size=args['batch_size'], \n",
        "                          shuffle=True, \n",
        "                          num_workers=args['num_workers'])\n",
        "# data loader não pode ser indexado\n",
        "# precisa de comando de iteração\n",
        "# para construir um batch a amostra deve ser tensorial\n",
        "for batch in treino_loader:\n",
        "  \n",
        "  dado, rotulo = batch\n",
        "  print(dado.size(), rotulo.size())\n",
        "\n",
        "  plt.imshow(dado[0][0])\n",
        "  plt.title('Rotulo: '+ str(rotulo[0]) )\n",
        "  break\n",
        "  ## 20 amostra de tamanho 28 por 28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "IMf2hLAH5NKL",
        "outputId": "a3d3a2b1-958b-46b3-ac47-563c743157d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 1, 28, 28]) torch.Size([20])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fazendo o fluxo de treinamento completo"
      ],
      "metadata": {
        "id": "yKqIrP3h7bTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementando MLP\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.features  = nn.Sequential(\n",
        "                      nn.Linear(input_size, hidden_size),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_size, hidden_size),\n",
        "                      nn.ReLU()\n",
        "                    )\n",
        "    self.out     = nn.Linear(hidden_size, out_size)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, X):\n",
        "    \n",
        "    X = X.view(X.size(0), -1)\n",
        "\n",
        "    feature = self.features(X)\n",
        "    output  = self.softmax(self.out(feature))\n",
        "\n",
        "    return output\n",
        "\n",
        "input_size  = 28 * 28\n",
        "hidden_size = 128\n",
        "out_size    = 10 #classes\n",
        "\n",
        "torch.manual_seed(42)\n",
        "net = MLP(input_size, hidden_size, out_size).to(args['device']) #cast na GPU "
      ],
      "metadata": {
        "id": "vhBiAEQq7pLr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(args['device'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "metadata": {
        "id": "5vE01v7VBaTM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fluxo\n",
        "for epoch in range(args['num_epochs']):\n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss = []\n",
        "  for batch in treino_loader:\n",
        "    dado, rotulo = batch\n",
        "    # cast \n",
        "    dado   = dado.to(args['device'])\n",
        "    rotulo = rotulo.to(args['device'])\n",
        "    # foward\n",
        "    pred = net(dado)\n",
        "    loss = criterion(pred, rotulo)\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "    \n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  end = time.time()\n",
        "\n",
        "  print(\"Epoca %d, Loss: %.4f +\\- %.4f, Tempo: %.2f\" % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start) )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q88X--W8CN0L",
        "outputId": "2c3ef7f6-446b-4ba6-d2d0-32661d0482e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 0, Loss: 1.8603 +\\- 0.1100, Tempo: 25.74\n"
          ]
        }
      ]
    }
  ]
}